---
title: "A Human-AI Collaborative Approach for Designing Sound Awareness Systems (HACS)"
collection: publications
# category: manuscripts
permalink: /publication/2024-chi-human-ai-sound-awareness
excerpt: 'This paper presents a Human-AI collaborative approach to designing sound awareness systems for Deaf and Hard-of-Hearing individuals.'
date: 2024-04-01
venue: 'CHI'
paperurl: '/files/HACS.pdf'
citation: 'Jeremy Zhengqi Huang, Reyna Wood, Hriday Chhabria, and Dhruv Jain. (2024). &quot;A Human-AI Collaborative Approach for Designing Sound Awareness Systems.&quot; <i>CHI 2024</i>. ACM, Article 884, 1–11.'
---

[ACM DL](https://doi.org/10.1145/3613904.3642062)
[PDF](/files/HACS.pdf)

**Abstract**  
HACS is a novel sound awareness system that classifies sounds based on their characteristics (e.g., a beep) and encourages DHH users to apply contextual knowledge (e.g., location) to recognize specific sound events (e.g., a microwave). For HACS, we collaborated with American Sign Language interpreters to develop a novel sound classification taxonomy and evaluated the system using qualitative input from DHH individuals and a sound recognition model, which demonstrated HACS's promise for creating a more accurate and reliable human-AI sound awareness system.

**Key Contributions**  
- For this project, I initially generated a similarity matrix from ASL interpreters’ sorting of different sound classes to hierarchically cluster our novel taxonomy.

- I also generated Mel-Spectrograms using Python to help validate the Convolutional Neural Network (CNN) we used to ensure the algorithmic distinguishability of different sounds in our taxonomy.

